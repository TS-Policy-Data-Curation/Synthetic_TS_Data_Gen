{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_to_numpy(file_path, delimiter=',', dtype=np.float32, skip_header=1, usecols=None):\n",
    "    \"\"\"\n",
    "    Load a CSV file into a NumPy array, skipping headers and selecting specific columns.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "        delimiter (str): Delimiter used in the CSV file. Default is ','.\n",
    "        dtype (data-type): Data type of the resulting array. Default is np.float32.\n",
    "        skip_header (int): Number of header rows to skip. Default is 1.\n",
    "        usecols (list or int): Columns to load. Default is None (load all columns).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: NumPy array containing the data from the CSV file.\n",
    "    \"\"\"\n",
    "    # Load the CSV file into a NumPy array, skipping headers and selecting specific columns\n",
    "    data = np.loadtxt(file_path, delimiter=delimiter, dtype=dtype, skiprows=skip_header, usecols=usecols)\n",
    "    return data\n",
    "\n",
    "def load_csv_to_tensor(file_path, delimiter=',', dtype=torch.float32, skip_header=1, usecols=None):\n",
    "    \"\"\"\n",
    "    Load a CSV file into a PyTorch tensor, skipping headers and selecting specific columns.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "        delimiter (str): Delimiter used in the CSV file. Default is ','.\n",
    "        dtype (torch.dtype): Data type of the resulting tensor. Default is torch.float32.\n",
    "        skip_header (int): Number of header rows to skip. Default is 1.\n",
    "        usecols (list or int): Columns to load. Default is None (load all columns).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: PyTorch tensor containing the data from the CSV file.\n",
    "    \"\"\"\n",
    "    # Load the CSV file into a NumPy array\n",
    "    data = load_csv_to_numpy(file_path, delimiter=delimiter, dtype=np.float32, skip_header=skip_header, usecols=usecols)\n",
    "    \n",
    "    # Convert the NumPy array to a PyTorch tensor\n",
    "    tensor = torch.tensor(data, dtype=dtype)\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "def load_csv_with_pandas(file_path, delimiter=','):\n",
    "    \"\"\"\n",
    "    Load a CSV file into a pandas DataFrame for more flexible handling.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "        delimiter (str): Delimiter used in the CSV file. Default is ','.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the data from the CSV file.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_path, delimiter=delimiter)\n",
    "\n",
    "def pandas_to_numpy(df, columns=None, dtype=np.float32):\n",
    "    \"\"\"\n",
    "    Convert specific columns of a pandas DataFrame to a NumPy array.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        columns (list or str): Columns to convert. Default is None (all numeric columns).\n",
    "        dtype (data-type): Data type of the resulting array. Default is np.float32.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: NumPy array containing the selected columns.\n",
    "    \"\"\"\n",
    "    if columns is not None:\n",
    "        df = df[columns]\n",
    "    return df.to_numpy(dtype=dtype)\n",
    "\n",
    "def pandas_to_tensor(df, columns=None, dtype=torch.float32):\n",
    "    \"\"\"\n",
    "    Convert specific columns of a pandas DataFrame to a PyTorch tensor.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        columns (list or str): Columns to convert. Default is None (all numeric columns).\n",
    "        dtype (torch.dtype): Data type of the resulting tensor. Default is torch.float32.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: PyTorch tensor containing the selected columns.\n",
    "    \"\"\"\n",
    "    if columns is not None:\n",
    "        df = df[columns]\n",
    "    return torch.tensor(df.to_numpy(), dtype=dtype)\n",
    "\n",
    "def monthly_inflation_rate(annual_rate):\n",
    "    \"\"\"\n",
    "    Calculate the monthly inflation rate given the annual inflation rate.\n",
    "\n",
    "    Parameters:\n",
    "    annual_rate (float): The annual inflation rate in decimal form (e.g., 0.12 for 12%).\n",
    "\n",
    "    Returns:\n",
    "    float: The monthly inflation rate in decimal form.\n",
    "    \"\"\"\n",
    "    monthly_rate = (1 + annual_rate) ** (1 / 12) - 1\n",
    "    return monthly_rate\n",
    "\n",
    "# # Example usage:\n",
    "# file_path = '/home/vincentwork/Synthetic_TS_Data_Gen/data/Real_GDP.csv'\n",
    "\n",
    "# # Using pandas to load the CSV file\n",
    "# df = load_csv_with_pandas(file_path)\n",
    "\n",
    "# # Convert specific columns to NumPy arrays or PyTorch tensors\n",
    "# dates = df['observation_date'].values  # Convert dates to a NumPy array\n",
    "# gdp_values_numpy = pandas_to_numpy(df, columns=['GDPC1'])  # Convert GDP values to a NumPy array\n",
    "# gdp_values_tensor = pandas_to_tensor(df, columns=['GDPC1'])  # Convert GDP values to a PyTorch tensor\n",
    "\n",
    "# print(\"DataFrame:\")\n",
    "# print(df)\n",
    "\n",
    "# print(\"\\nDates (NumPy Array):\")\n",
    "# print(dates)\n",
    "\n",
    "# print(\"\\nGDP Values (NumPy Array):\")\n",
    "# print(gdp_values_numpy)\n",
    "\n",
    "# print(\"\\nGDP Values (PyTorch Tensor):\")\n",
    "# print(gdp_values_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: unemployment.csv\n",
      "Loading file: inflation.csv\n",
      "Loading file: Real_GDP.csv\n"
     ]
    }
   ],
   "source": [
    "# Loop through the data directory and load all the csv files\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "data_directory = '/home/vincentwork/Synthetic_TS_Data_Gen/data/'\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for file in os.listdir(data_directory):\n",
    "    if file.endswith(\".csv\"):\n",
    "        print(f\"Loading file: {file}\")\n",
    "        # Load the CSV file into a DataFrame\n",
    "        file_path = os.path.join(data_directory, file)\n",
    "        df = load_csv_with_pandas(file_path)\n",
    "        \n",
    "        # Convert the 'observation_date' column to datetime format\n",
    "        df['observation_date'] = pd.to_datetime(df['observation_date'])\n",
    "        \n",
    "        feature = df.columns[-1]\n",
    "        \n",
    "        if feature == 'FPCPITOTLZGUSA':\n",
    "            expanded_data = []\n",
    "            for index, row in df.iterrows():\n",
    "                year = row['observation_date'].year\n",
    "                value = row[feature]\n",
    "                montly_value = monthly_inflation_rate(value)\n",
    "                for month in range(1, 13):\n",
    "                    expanded_data.append({\n",
    "                        'observation_date': datetime(year, month, 1),  \n",
    "                        feature: montly_value\n",
    "                    })\n",
    "            expanded_df = pd.DataFrame(expanded_data)\n",
    "            df = expanded_df.sort_values(by='observation_date').reset_index(drop=True)\n",
    "        \n",
    "        if feature == 'GDPC1':\n",
    "            df.set_index('observation_date', inplace=True)\n",
    "            monthly_df = df.resample('MS').interpolate(method='linear')\n",
    "            monthly_df.reset_index(inplace=True)\n",
    "            df = monthly_df\n",
    "            \n",
    "        # Append the DataFrame to the list\n",
    "        dataframes.append(df)\n",
    "\n",
    "        # Merge all DataFrames on the 'observation_date' column\n",
    "        merged_df = dataframes[0]  # Start with the first DataFrame\n",
    "\n",
    "\n",
    "for df in dataframes[1:]:\n",
    "    merged_df = pd.merge(merged_df, df, on='observation_date', how='outer')\n",
    "\n",
    "# Sort the merged DataFrame by 'observation_date'\n",
    "merged_df = merged_df.sort_values(by='observation_date')\n",
    "\n",
    "# Fill in NaN data with the previous value\n",
    "# merged_df = merged_df.fillna(method='ffill')\n",
    "\n",
    "merged_df.drop(columns=['observation_date'], inplace=True)\n",
    "\n",
    "merged_df.dropna(inplace=True)\n",
    "\n",
    "merged_df.to_csv('/home/vincentwork/Synthetic_TS_Data_Gen/data/diffusion3.csv', index=False)\n",
    "\n",
    "\n",
    "headers = [col for col in merged_df.columns if col != 'observation_date']\n",
    "\n",
    "# Drop nan values (rows)\n",
    "# merged_df = merged_df.dropna()\n",
    "\n",
    "# Convert to Numpy array\n",
    "# Select only the columns containing the time series data\n",
    "# time_series_numpy = pandas_to_numpy(merged_df, columns=headers)\n",
    "# print(time_series_numpy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of time_VAE_format: (64, 12, 3)\n",
      "[[[5.1999998e+00 7.7824712e-02 3.5171809e+03]\n",
      "  [4.8000002e+00 7.7824712e-02 3.5108694e+03]\n",
      "  [5.4000001e+00 7.7824712e-02 3.5045576e+03]\n",
      "  ...\n",
      "  [6.0999999e+00 7.7824712e-02 3.4702781e+03]\n",
      "  [6.0999999e+00 7.7824712e-02 3.4780864e+03]\n",
      "  [6.5999999e+00 7.7824712e-02 3.4858948e+03]]\n",
      "\n",
      " [[6.5999999e+00 6.2535673e-02 3.4937029e+03]\n",
      "  [6.9000001e+00 6.2535673e-02 3.5134756e+03]\n",
      "  [6.9000001e+00 6.2535673e-02 3.5332483e+03]\n",
      "  ...\n",
      "  [6.5000000e+00 6.2535673e-02 3.6922891e+03]\n",
      "  [6.0999999e+00 6.2535673e-02 3.7142417e+03]\n",
      "  [6.0000000e+00 6.2535673e-02 3.7361943e+03]]\n",
      "\n",
      " [[5.8000002e+00 6.7861773e-02 3.7581470e+03]\n",
      "  [5.5000000e+00 6.7861773e-02 3.7694810e+03]\n",
      "  [5.5999999e+00 6.7861773e-02 3.7808149e+03]\n",
      "  ...\n",
      "  [5.4000001e+00 6.7861773e-02 3.8514209e+03]\n",
      "  [5.6999998e+00 6.7861773e-02 3.8654414e+03]\n",
      "  [5.5000000e+00 6.7861773e-02 3.8794617e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[6.4000001e+00 1.5604828e-01 2.1058379e+04]\n",
      "  [6.1999998e+00 1.5604828e-01 2.1168588e+04]\n",
      "  [6.0999999e+00 1.5604828e-01 2.1278797e+04]\n",
      "  ...\n",
      "  [4.5000000e+00 1.5604828e-01 2.1960389e+04]\n",
      "  [4.0999999e+00 1.5604828e-01 2.1941543e+04]\n",
      "  [3.9000001e+00 1.5604828e-01 2.1922695e+04]]\n",
      "\n",
      " [[4.0000000e+00 2.0096809e-01 2.1903850e+04]\n",
      "  [3.8000000e+00 2.0096809e-01 2.1908975e+04]\n",
      "  [3.5999999e+00 2.0096809e-01 2.1914098e+04]\n",
      "  ...\n",
      "  [3.5999999e+00 2.0096809e-01 2.2249459e+04]\n",
      "  [3.5999999e+00 2.0096809e-01 2.2300785e+04]\n",
      "  [3.5000000e+00 2.0096809e-01 2.2352109e+04]]\n",
      "\n",
      " [[3.4000001e+00 1.4572380e-01 2.2403436e+04]\n",
      "  [3.5999999e+00 1.4572380e-01 2.2448762e+04]\n",
      "  [3.5000000e+00 1.4572380e-01 2.2494090e+04]\n",
      "  ...\n",
      "  [3.8000000e+00 1.4572380e-01 2.2960600e+04]\n",
      "  [3.7000000e+00 1.4572380e-01 2.2991582e+04]\n",
      "  [3.7000000e+00 1.4572380e-01 2.3022562e+04]]]\n"
     ]
    }
   ],
   "source": [
    "### Convert to time_VAE formatting ###\n",
    "# time_VAE formatting: [samples, timestamps, features_list]\n",
    "# TODO: How do I get more samples? \n",
    "\n",
    "# Reshape the array into the format [samples, timestamps, features_list]\n",
    "# Since there is only 1 sample, we add an extra dimension at the beginning\n",
    "# num_timestamps = time_series_numpy.shape[0]  # Number of timestamps\n",
    "# num_features = time_series_numpy.shape[1]    # Number of features\n",
    "\n",
    "# Reshape to [1, timestamps, features]\n",
    "# time_VAE_format = time_series_numpy.reshape(1, num_timestamps, num_features)\n",
    "\n",
    "# Reshape to [num years (samples), timestamps (monthly), features (unemployment, inflation, GDP)]\n",
    "# Extract year from observation_date\n",
    "merged_df['year'] = merged_df['observation_date'].dt.year\n",
    "\n",
    "# Split the DataFrame by year into a dictionary of DataFrames\n",
    "yearly_dict = {year: group.drop(columns=['year']) for year, group in merged_df.groupby('year')}\n",
    "\n",
    "# Convert each yearly DataFrame to a 2D numpy array and store in a list\n",
    "yearly_arrays = [pandas_to_numpy(year_df, headers) for year, year_df in yearly_dict.items()]\n",
    "\n",
    "# Combine the 2D arrays into a 3D numpy array\n",
    "time_series_numpy = np.stack(yearly_arrays)\n",
    "    \n",
    "\n",
    "# Print the shape of the resulting array\n",
    "print(\"Shape of time_VAE_format:\", time_series_numpy.shape)\n",
    "print(time_series_numpy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###Data Augmentation for more Samples###\n",
    "# # Original time series data (1 sample, timestamps, features)\n",
    "# original_data = time_series_numpy\n",
    "\n",
    "# # Number of augmented samples to create\n",
    "# num_augmented_samples = 1000\n",
    "\n",
    "# # Initialize a list to store augmented samples\n",
    "# augmented_data = []\n",
    "\n",
    "# for _ in range(num_augmented_samples):\n",
    "#     # Add random noise to the original data\n",
    "#     noise = np.random.normal(0, 0.05, original_data.shape)  # Adjust noise level as needed\n",
    "#     augmented_sample = original_data + noise\n",
    "    \n",
    "#     # Append the augmented sample to the list\n",
    "#     augmented_data.append(augmented_sample)\n",
    "\n",
    "# # Convert the list to a numpy array\n",
    "# augmented_data = np.concatenate(augmented_data, axis=0)\n",
    "\n",
    "# print(\"Augmented data shape:\", augmented_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to /home/vincentwork/Synthetic_TS_Data_Gen/data/macroeconomic.npz\n"
     ]
    }
   ],
   "source": [
    "###Save the data to a .npz file###\n",
    "def save_numpy_to_npz(data, npz_file_path):\n",
    "    \"\"\"\n",
    "    Save a NumPy array directly into a compressed .npz file.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): The NumPy array to save.\n",
    "        npz_file_path (str): Path to save the compressed .npz file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Save the NumPy array as a compressed .npz file\n",
    "    np.savez_compressed(npz_file_path, data=data)\n",
    "    \n",
    "    print(f\"Data successfully saved to {npz_file_path}\")\n",
    "\n",
    "data_directory = '/home/vincentwork/Synthetic_TS_Data_Gen/data/'\n",
    "npz_file_name = \"macroeconomic.npz\"\n",
    "npz_file_path = os.path.join(data_directory, npz_file_name)\n",
    "\n",
    "save_numpy_to_npz(time_series_numpy, npz_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
